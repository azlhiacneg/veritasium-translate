1
00:00:00,230 --> 00:00:02,840
- Alright, I'm about to
go for my first ever ride

2
00:00:02,840 --> 00:00:05,340
in a fully autonomous vehicle.

3
00:00:09,890 --> 00:00:11,583
Whoa, no driver.

4
00:00:14,230 --> 00:00:15,063
All right.

5
00:00:15,063 --> 00:00:16,410
- [Electronic Voice] Good morning, Derek.

6
00:00:16,410 --> 00:00:19,220
This car is all yours
with no one up front.

7
00:00:19,220 --> 00:00:22,967
- I really like the idea of
fully autonomous vehicles,

8
00:00:22,967 --> 00:00:26,890
but it's weird getting
into a car with no driver

9
00:00:26,890 --> 00:00:28,940
and just trusting the car.

10
00:00:28,940 --> 00:00:32,310
I'm gonna report back how this ride goes

11
00:00:32,310 --> 00:00:34,210
and how I feel about it.

12
00:00:34,210 --> 00:00:37,453
Oh, but full disclosure, this
video is sponsored by Waymo.

13
00:00:41,585 --> 00:00:43,274
(car door clicking)

14
00:00:43,274 --> 00:00:44,107
(seatbelt clicking)

15
00:00:44,107 --> 00:00:46,577
Start ride.
(car navigation chiming)

16
00:00:46,577 --> 00:00:47,410
Ha ha!

17
00:00:47,410 --> 00:00:49,127
- [Electronic Voice] Make sure
your seatbelt is fastened.

18
00:00:49,127 --> 00:00:50,450
(Derek vocalizes)
For any questions,

19
00:00:50,450 --> 00:00:53,930
press the Help button to speak
with a rider support agent.

20
00:00:53,930 --> 00:00:56,630
- Okay. Now let's see where we go.

21
00:00:56,630 --> 00:00:59,480
It's looking at this car coming
here. What's it gonna do?

22
00:01:00,590 --> 00:01:04,070
Oh, and it pulls in
very smoothly behind it.

23
00:01:04,070 --> 00:01:06,430
No problems. Did not turn into traffic.

24
00:01:06,430 --> 00:01:09,460
Waited until the cars
went, and then it turned.

25
00:01:09,460 --> 00:01:10,293
I like that.

26
00:01:11,350 --> 00:01:14,250
I polled YouTube viewers
about autonomous vehicles

27
00:01:14,250 --> 00:01:16,500
and half of you are excited

28
00:01:16,500 --> 00:01:18,500
and ready for them to be on the roads,

29
00:01:18,500 --> 00:01:21,350
but over 40% said you
thought the technology

30
00:01:21,350 --> 00:01:24,010
was still over 10 years away.

31
00:01:24,010 --> 00:01:26,990
And for those people, I have news,

32
00:01:26,990 --> 00:01:30,420
which is that, well, there
is no driver in this car.

33
00:01:30,420 --> 00:01:34,580
I'm currently inside a fully
autonomous vehicle driving

34
00:01:34,580 --> 00:01:36,903
around a suburb of Phoenix, Arizona.

35
00:01:37,880 --> 00:01:39,930
Now I get that in some parts of the world,

36
00:01:39,930 --> 00:01:41,960
like, the roads aren't
well enough maintained

37
00:01:41,960 --> 00:01:44,570
and people don't stay in
their lane necessarily,

38
00:01:44,570 --> 00:01:46,850
and so it'd be very hard for
a computer to drive there,

39
00:01:46,850 --> 00:01:49,090
but at least under good conditions,

40
00:01:49,090 --> 00:01:51,583
the technology is currently functional.

41
00:01:51,583 --> 00:01:54,416
(downtempo music)

42
00:01:57,104 --> 00:01:58,840
(bright comedic music)

43
00:01:58,840 --> 00:02:00,170
- Now Waymo started out

44
00:02:00,170 --> 00:02:02,910
as the Google self-driving car project

45
00:02:02,910 --> 00:02:05,783
with what is possibly one of
the cutest cars ever made.

46
00:02:08,770 --> 00:02:12,980
I am inside the world's first
fully autonomous vehicle.

47
00:02:12,980 --> 00:02:17,770
Back in October 2015, this
car went on a public road,

48
00:02:17,770 --> 00:02:19,360
ridden by Steve Mann,

49
00:02:19,360 --> 00:02:22,180
who has a disability, he is legally blind,

50
00:02:22,180 --> 00:02:25,180
but he could get around in this thing,

51
00:02:25,180 --> 00:02:28,600
which is affectionately
known as the Firefly.

52
00:02:28,600 --> 00:02:30,300
This is such a simple vehicle.

53
00:02:30,300 --> 00:02:32,360
There's basically nothing in here.

54
00:02:32,360 --> 00:02:34,640
There's no steering wheel, no dashboard.

55
00:02:34,640 --> 00:02:36,170
This car is super basic.

56
00:02:36,170 --> 00:02:40,700
There is no AC, but there
is an emergency stop button.

57
00:02:40,700 --> 00:02:41,533
(button cover clicks)

58
00:02:41,533 --> 00:02:43,260
That's my favorite button in the car.

59
00:02:43,260 --> 00:02:45,370
It reminds me of elevators.

60
00:02:45,370 --> 00:02:47,410
One of the important
measures that they had to put

61
00:02:47,410 --> 00:02:51,150
in automatic elevators
was a big red stop button.

62
00:02:51,150 --> 00:02:53,610
Did you know that before the 1940s,

63
00:02:53,610 --> 00:02:56,710
almost all elevators had drivers in them?

64
00:02:56,710 --> 00:03:00,370
And when people started putting
in driverless elevators,

65
00:03:00,370 --> 00:03:02,160
well, the public was very concerned,

66
00:03:02,160 --> 00:03:03,910
and they didn't wanna
ride in those elevators.

67
00:03:03,910 --> 00:03:04,997
There was one guy who was like,

68
00:03:04,997 --> 00:03:07,930
"I don't care if I have to
walk up 12 flights of stairs

69
00:03:07,930 --> 00:03:08,980
for the rest of my life.

70
00:03:08,980 --> 00:03:10,560
I'm not taking that elevator."

71
00:03:10,560 --> 00:03:11,550
And adoption was slow.

72
00:03:11,550 --> 00:03:12,840
I mean, they tried to advertise

73
00:03:12,840 --> 00:03:15,410
to help people understand
that it was in fact safe,

74
00:03:15,410 --> 00:03:18,810
but, ultimately, there was
an elevator drivers strike

75
00:03:18,810 --> 00:03:21,800
in New York City, and that
really annoyed people,

76
00:03:21,800 --> 00:03:25,170
and it helped the adoption
of automated elevators.

77
00:03:25,170 --> 00:03:27,370
If you found a driver
in an elevator today,

78
00:03:27,370 --> 00:03:30,050
you would wonder, "Why are they there?"

79
00:03:30,050 --> 00:03:32,660
Now you might think an
elevator is just so simple,

80
00:03:32,660 --> 00:03:35,390
I mean, it is effectively
one dimensional motion,

81
00:03:35,390 --> 00:03:40,361
but airplanes are also flown
extensively by computers.

82
00:03:40,361 --> 00:03:42,110
(tense percussive music)

83
00:03:42,110 --> 00:03:44,550
I saw this particular landing

84
00:03:44,550 --> 00:03:47,100
where a plane is coming in into Vienna,

85
00:03:47,100 --> 00:03:48,590
and it's just so foggy

86
00:03:48,590 --> 00:03:50,370
that the pilots can see almost nothing.

87
00:03:50,370 --> 00:03:53,210
I mean, this is the view from the cockpit,

88
00:03:53,210 --> 00:03:55,680
and yet, they make a perfect,

89
00:03:55,680 --> 00:03:58,340
a textbook landing right on target.

90
00:03:58,340 --> 00:04:00,360
So how do they do it?

91
00:04:00,360 --> 00:04:03,100
The answer is the pilots didn't do it.

92
00:04:03,100 --> 00:04:05,820
It was a CAT II autoland procedure.

93
00:04:05,820 --> 00:04:09,990
The plane just came in and
landed itself essentially.

94
00:04:09,990 --> 00:04:11,840
Now, of course, the pilots are important

95
00:04:11,840 --> 00:04:15,480
and they're monitoring all of
the instruments and controls,

96
00:04:15,480 --> 00:04:17,010
but it's actually the plane

97
00:04:17,010 --> 00:04:20,400
and its computer getting the
plane to land appropriately.

98
00:04:20,400 --> 00:04:24,230
I was surprised to learn that
humans are much more likely

99
00:04:24,230 --> 00:04:28,050
to take manual control
and land on sunny days,

100
00:04:28,050 --> 00:04:32,600
like July 6th, 2013, when
Asiana Airlines flight 214

101
00:04:32,600 --> 00:04:35,420
was on final approach to San Francisco.

102
00:04:35,420 --> 00:04:37,620
Attempting to manually land the plane,

103
00:04:37,620 --> 00:04:40,960
the pilot accidentally
left the throttle at zero,

104
00:04:40,960 --> 00:04:42,210
and by the time they realized

105
00:04:42,210 --> 00:04:45,243
and tried to abort the
landing, it was too late.

106
00:04:46,500 --> 00:04:51,070
The plane crashed into the
runway seawall and split in two.

107
00:04:51,070 --> 00:04:54,483
Three people died in the
aftermath of this accident.

108
00:04:55,600 --> 00:04:57,680
I think the counterintuitive thing

109
00:04:57,680 --> 00:05:00,540
is that we expect the humans to be better,

110
00:05:00,540 --> 00:05:02,430
particularly in tough situations,

111
00:05:02,430 --> 00:05:05,320
but when it comes to
airplanes, if it's bad weather,

112
00:05:05,320 --> 00:05:08,633
you actually want the plane flying itself.

113
00:05:09,840 --> 00:05:11,990
So the obvious next question is,

114
00:05:11,990 --> 00:05:13,535
would you want the same thing for cars?

115
00:05:13,535 --> 00:05:15,330
(oscillating music)

116
00:05:15,330 --> 00:05:17,570
There are all these
different levels of autonomy,

117
00:05:17,570 --> 00:05:20,780
and everything up to four
requires a human driver

118
00:05:20,780 --> 00:05:24,370
to be responsible and have
the wheel at all times.

119
00:05:24,370 --> 00:05:27,110
In the early days of the Google
self-driving car project,

120
00:05:27,110 --> 00:05:29,510
they had a vehicle that
was not yet level four,

121
00:05:29,510 --> 00:05:31,590
so it still required a human driver.

122
00:05:31,590 --> 00:05:34,280
They let Google employees borrow the cars,

123
00:05:34,280 --> 00:05:36,830
but they still had to be
in control of the wheel.

124
00:05:36,830 --> 00:05:38,340
And the volunteers were informed

125
00:05:38,340 --> 00:05:41,210
that they were responsible
for the car at all times

126
00:05:41,210 --> 00:05:42,890
and that they would be
constantly recorded,

127
00:05:42,890 --> 00:05:45,630
like video recorded, while
they were in the car.

128
00:05:45,630 --> 00:05:47,810
But still, within a short period of time,

129
00:05:47,810 --> 00:05:50,240
the engineers observed
drivers rummaging around

130
00:05:50,240 --> 00:05:53,900
in their bags or checking
phones, putting on makeup,

131
00:05:53,900 --> 00:05:57,480
or even sleeping in the driver's seat.

132
00:05:57,480 --> 00:06:00,540
All these drivers were trusting
the technology too much,

133
00:06:00,540 --> 00:06:03,120
which makes almost fully
autonomous vehicles

134
00:06:03,120 --> 00:06:06,360
potentially more dangerous
than regular cars,

135
00:06:06,360 --> 00:06:07,910
I mean, if the driver is distracted

136
00:06:07,910 --> 00:06:10,140
or not prepared to take over.

137
00:06:10,140 --> 00:06:12,460
So this is why Waymo decided

138
00:06:12,460 --> 00:06:14,840
that the only safe way
to proceed is with a car

139
00:06:14,840 --> 00:06:16,833
that has at least level four autonomy.

140
00:06:21,240 --> 00:06:23,150
This is the depot where the cars go

141
00:06:23,150 --> 00:06:24,417
when they're not on the road.

142
00:06:24,417 --> 00:06:26,620
And it's also where people monitor

143
00:06:26,620 --> 00:06:28,550
all the rides in progress.

144
00:06:28,550 --> 00:06:30,590
- Yes, so that's where my team sits.

145
00:06:30,590 --> 00:06:32,687
You see three teams basically here.

146
00:06:32,687 --> 00:06:35,460
One of the teams is my
team of fleet dispatchers,

147
00:06:35,460 --> 00:06:36,770
so basically making sure

148
00:06:36,770 --> 00:06:39,310
that all the mission
are assigned every day

149
00:06:39,310 --> 00:06:41,330
and they are completed
successfully on the road.

150
00:06:41,330 --> 00:06:42,740
And then you have the Rider Support team

151
00:06:42,740 --> 00:06:43,790
that takes the calls.

152
00:06:45,661 --> 00:06:46,530
(electronic tone chiming)

153
00:06:46,530 --> 00:06:48,628
- [Beulah] Thank you for
calling the Waymo Rider Support.

154
00:06:48,628 --> 00:06:50,630
This is Beulah, how can I help today?

155
00:06:50,630 --> 00:06:51,960
- I just completed my ride,

156
00:06:51,960 --> 00:06:53,610
but I don't wanna get out of the car.

157
00:06:53,610 --> 00:06:54,820
I just wanna keep driving.

158
00:06:54,820 --> 00:06:55,970
Is there a way that I can do that?

159
00:06:55,970 --> 00:06:58,130
- [Beulah] Right now, I
don't see a trip started.

160
00:06:58,130 --> 00:07:00,900
Give me one moment here while
I partner with my team, okay?

161
00:07:00,900 --> 00:07:01,733
- Okay.

162
00:07:06,397 --> 00:07:11,170
(chuckling) Well, I just
left all my stuff in the car.

163
00:07:11,170 --> 00:07:12,413
I hope it comes back.

164
00:07:17,211 --> 00:07:18,980
(car humming)

165
00:07:18,980 --> 00:07:21,740
- I think there's a lot
of, still, resistance

166
00:07:21,740 --> 00:07:24,370
in terms of trusting the vehicle.

167
00:07:24,370 --> 00:07:26,147
And they ask you like,

168
00:07:26,147 --> 00:07:29,080
"How does it feel to be in
a car without the driver?"

169
00:07:29,080 --> 00:07:33,090
I was the first person to do public roads,

170
00:07:33,090 --> 00:07:35,370
fully driverless ride at night.

171
00:07:35,370 --> 00:07:36,760
I always share the experience with them.

172
00:07:36,760 --> 00:07:40,180
It takes about two minutes
for you to completely forget

173
00:07:40,180 --> 00:07:42,800
that you are in a driverless vehicle.

174
00:07:42,800 --> 00:07:47,040
If the system really provides
that feeling that you're safe

175
00:07:47,040 --> 00:07:48,930
and you see a couple of maneuvers,

176
00:07:48,930 --> 00:07:49,950
in less than two minutes,

177
00:07:49,950 --> 00:07:51,790
you're talking to whoever is next to you

178
00:07:51,790 --> 00:07:54,423
and not paying attention to
what's happening anymore.

179
00:07:56,720 --> 00:08:00,270
- Whoa, all right, it doesn't
make the indicator sound,

180
00:08:00,270 --> 00:08:04,280
so I just don't know
when it's going to turn,

181
00:08:04,280 --> 00:08:06,483
but if I was watching
the map, I would know.

182
00:08:08,770 --> 00:08:11,070
I think we have this bias to believe

183
00:08:11,070 --> 00:08:14,290
that we're better at certain
tasks than we actually are,

184
00:08:14,290 --> 00:08:17,290
like thinking that people
are good at driving.

185
00:08:17,290 --> 00:08:20,480
Surveys show 74% of people believe

186
00:08:20,480 --> 00:08:23,870
they are above average drivers.

187
00:08:23,870 --> 00:08:25,230
Think about that.

188
00:08:25,230 --> 00:08:26,170
In the 20th century,

189
00:08:26,170 --> 00:08:28,600
60 million people were killed on the road.

190
00:08:28,600 --> 00:08:32,290
That's basically an extra
world war's worth of deaths.

191
00:08:32,290 --> 00:08:34,970
And we really have no one
to blame but ourselves.

192
00:08:34,970 --> 00:08:36,710
The National Transportation
and Safety Board

193
00:08:36,710 --> 00:08:41,480
has identified human error as
the cause of 94% of accidents.

194
00:08:41,480 --> 00:08:45,233
Most of these errors are
impossible for a machine to make.

195
00:08:46,210 --> 00:08:48,690
Every year when people are backing out

196
00:08:48,690 --> 00:08:51,260
of driveways or parking spaces,

197
00:08:51,260 --> 00:08:54,640
in the U.S, up to 200 people are killed,

198
00:08:54,640 --> 00:08:57,380
and it's frequently
older people or children,

199
00:08:57,380 --> 00:08:59,370
the children of the drivers.

200
00:08:59,370 --> 00:09:00,630
It's awful.

201
00:09:00,630 --> 00:09:01,810
And it comes down to the fact

202
00:09:01,810 --> 00:09:03,960
that we don't have eyes
in the back of our head,

203
00:09:03,960 --> 00:09:07,360
and even the backup cameras
still have blind spots.

204
00:09:07,360 --> 00:09:10,170
But if you have a vehicle that has LiDAR

205
00:09:10,170 --> 00:09:13,280
and radar and 29 cameras,

206
00:09:13,280 --> 00:09:16,450
you're just not going to hit them.

207
00:09:16,450 --> 00:09:21,450
Up here in the very prominent
top, there is a 360 LiDAR.

208
00:09:21,480 --> 00:09:23,790
So it can see all around the car.

209
00:09:23,790 --> 00:09:27,340
It can see up to 300
meters away with a LiDAR.

210
00:09:27,340 --> 00:09:29,080
The way the LiDAR works

211
00:09:29,080 --> 00:09:32,100
is it shoots out invisible laser beams,

212
00:09:32,100 --> 00:09:35,080
scanning around millions
of times a second,

213
00:09:35,080 --> 00:09:37,050
and then it detects the reflection,

214
00:09:37,050 --> 00:09:39,420
and how long it takes
to come back allows you

215
00:09:39,420 --> 00:09:42,120
to determine how far it is to that object.

216
00:09:42,120 --> 00:09:45,820
So what it's doing is
like painting a 3D picture

217
00:09:45,820 --> 00:09:47,090
of the world.

218
00:09:47,090 --> 00:09:50,990
There are 29 cameras around this vehicle,

219
00:09:50,990 --> 00:09:53,550
which gives you full 360 vision.

220
00:09:53,550 --> 00:09:55,450
It gives you close range vision,

221
00:09:55,450 --> 00:09:56,800
what is right next to the car,

222
00:09:56,800 --> 00:10:01,080
and also long range vision,
going out 500 meters.

223
00:10:01,080 --> 00:10:02,810
This car could detect a stop sign

224
00:10:02,810 --> 00:10:05,610
or a pedestrian 500 meters away.

225
00:10:05,610 --> 00:10:08,580
How many of us have
eyesight that is that good?

226
00:10:08,580 --> 00:10:12,120
There is also a microphone up on top

227
00:10:12,120 --> 00:10:14,650
to listen to what's
happening in the environment,

228
00:10:14,650 --> 00:10:16,340
and if there are sirens,

229
00:10:16,340 --> 00:10:19,340
then the car will pull over
to the side of the road.

230
00:10:19,340 --> 00:10:22,692
It's gotta be able to respond
to emergency vehicles.

231
00:10:22,692 --> 00:10:26,109
(light percussive music)

232
00:10:31,520 --> 00:10:36,010
What I wanna see here is, how
does it handle a parking lot

233
00:10:36,010 --> 00:10:39,380
where there's people
driving in unusual ways

234
00:10:39,380 --> 00:10:42,218
and possibly pedestrians walking around?

235
00:10:42,218 --> 00:10:44,718
(car humming)

236
00:10:46,900 --> 00:10:48,220
(tires squeak)
Whoa.

237
00:10:48,220 --> 00:10:49,883
That was a sudden stop.

238
00:10:52,130 --> 00:10:53,730
The car made a pretty hard stop there.

239
00:10:53,730 --> 00:10:56,190
I think it saw that guy
with a cart coming up

240
00:10:56,190 --> 00:10:57,570
on a pedestrian crosswalk.

241
00:10:57,570 --> 00:10:59,240
And one of the interesting things

242
00:10:59,240 --> 00:11:01,040
that the vehicle is always doing

243
00:11:01,040 --> 00:11:03,280
is not only seeing where things are

244
00:11:03,280 --> 00:11:06,070
and where they're going,
but also making predictions

245
00:11:06,070 --> 00:11:08,140
about where they're likely to go.

246
00:11:08,140 --> 00:11:12,280
So this car doesn't just
have one potential future.

247
00:11:12,280 --> 00:11:13,847
It's constantly imagining,

248
00:11:13,847 --> 00:11:15,970
"Well, he might cross at the crosswalk,"

249
00:11:15,970 --> 00:11:18,640
or, "He might keep going,"
or, "He might turn left,"

250
00:11:18,640 --> 00:11:20,340
and so it has to be prepared

251
00:11:20,340 --> 00:11:22,170
for all of those different options.

252
00:11:22,170 --> 00:11:25,600
And it even weights the options
of like how likely it thinks

253
00:11:25,600 --> 00:11:27,070
that he's gonna go on the crosswalk

254
00:11:27,070 --> 00:11:29,380
versus go straight versus turn.

255
00:11:29,380 --> 00:11:32,100
And you can see that with
the thickness of the line

256
00:11:32,100 --> 00:11:34,450
in the little simulated
graphic that they have.

257
00:11:37,201 --> 00:11:38,530
(seatbelt clicking)
Phew.

258
00:11:38,530 --> 00:11:40,860
A few years back, I think a
lot of people were talking

259
00:11:40,860 --> 00:11:44,220
about how autonomous
vehicles have to figure out

260
00:11:44,220 --> 00:11:46,170
who to hit in case of an accident,

261
00:11:46,170 --> 00:11:49,090
like do the pick the orphan or the nun?

262
00:11:49,090 --> 00:11:51,630
Should the car hit the
motorcyclist with the helmet on

263
00:11:51,630 --> 00:11:53,910
because his injuries might be less severe

264
00:11:53,910 --> 00:11:55,540
or should the car hit the motorcyclist

265
00:11:55,540 --> 00:11:56,840
who does not have a helmet on

266
00:11:56,840 --> 00:11:59,000
because he did not
properly protect himself?

267
00:11:59,000 --> 00:12:00,010
If cars were programmed

268
00:12:00,010 --> 00:12:02,050
to hit the motorcyclist with the helmet,

269
00:12:02,050 --> 00:12:03,500
that would mean that, in a way,

270
00:12:03,500 --> 00:12:05,308
it would become safer to
ride without a helmet.

271
00:12:05,308 --> 00:12:06,190
(whimsical music)

272
00:12:06,190 --> 00:12:11,190
But the reality is that 99%
of accidents aren't like that.

273
00:12:11,840 --> 00:12:15,990
Every year, around 1.3
million people are killed

274
00:12:15,990 --> 00:12:19,200
on the roads, almost all
of them due to human error.

275
00:12:19,200 --> 00:12:22,250
If autonomous cars can
reduce these fatalities,

276
00:12:22,250 --> 00:12:23,680
then the real moral dilemma

277
00:12:23,680 --> 00:12:25,710
is not getting them on the road sooner

278
00:12:25,710 --> 00:12:28,380
for fear we haven't worked
out exactly how they'll react

279
00:12:28,380 --> 00:12:32,273
to extremely unlikely
hypothetical scenarios.

280
00:12:33,460 --> 00:12:36,420
I think humans are becoming worse drivers

281
00:12:36,420 --> 00:12:38,808
because we're just so
prone to distraction.

282
00:12:38,808 --> 00:12:39,641
(light contemplative music)

283
00:12:39,641 --> 00:12:44,250
Think about the main
reasons why cars crash:

284
00:12:44,250 --> 00:12:46,510
because people are speeding,

285
00:12:46,510 --> 00:12:50,220
they're under the influence,
they're distracted.

286
00:12:50,220 --> 00:12:53,220
I mean, these sorts of problems,

287
00:12:53,220 --> 00:12:55,350
an autonomous vehicle would not have.

288
00:12:55,350 --> 00:12:57,360
You don't get a distracted driver.

289
00:12:57,360 --> 00:12:59,700
The ultimate question, right,
that everyone wants to know,

290
00:12:59,700 --> 00:13:02,130
that I want to know the answer to is,

291
00:13:02,130 --> 00:13:03,820
as these vehicles stand,
(car electronic beeping)

292
00:13:03,820 --> 00:13:05,810
are they better than the average human?

293
00:13:05,810 --> 00:13:08,590
Not than the best human, but
just like an average human.

294
00:13:08,590 --> 00:13:11,000
Like replacing some
random car on the street

295
00:13:11,000 --> 00:13:13,860
with one of these vehicles,
does that make the road safer?

296
00:13:13,860 --> 00:13:14,920
- Yes, it does.

297
00:13:14,920 --> 00:13:19,840
I think we would never
launch a rider-only service

298
00:13:19,840 --> 00:13:23,290
if we did not meet that
base safety framework.

299
00:13:23,290 --> 00:13:27,170
- If that's true, it
means like every vehicle

300
00:13:27,170 --> 00:13:31,290
that's not on the road is
kind of a worse situation,

301
00:13:31,290 --> 00:13:33,160
do you know what I mean?
- We are really working

302
00:13:33,160 --> 00:13:36,870
really hard to launch this in larger areas

303
00:13:36,870 --> 00:13:39,780
and new areas too, but we
need to have the experience

304
00:13:39,780 --> 00:13:42,410
to show the regulators
why we believe it's safer,

305
00:13:42,410 --> 00:13:45,190
and, for that, you need
to be driving miles,

306
00:13:45,190 --> 00:13:48,360
a number of miles that
you feel comfortable with,

307
00:13:48,360 --> 00:13:50,120
statistically speaking.

308
00:13:50,120 --> 00:13:52,800
- These vehicles have way more experience

309
00:13:52,800 --> 00:13:56,300
than any human driver because
they've now accumulated data

310
00:13:56,300 --> 00:14:00,770
over 20 million miles of
driving on public roads.

311
00:14:00,770 --> 00:14:02,620
If you were an average driver,

312
00:14:02,620 --> 00:14:05,220
you'd have to drive for a thousand years

313
00:14:05,220 --> 00:14:07,140
to accumulate that sort of experience.

314
00:14:07,140 --> 00:14:09,660
And all of that experience can be used

315
00:14:09,660 --> 00:14:11,980
to train the systems, to fix the software,

316
00:14:11,980 --> 00:14:14,363
and used across all the
vehicles in the fleet.

317
00:14:15,770 --> 00:14:18,850
In 2019, Waymo released
a study of its data,

318
00:14:18,850 --> 00:14:21,940
over 6.1 million miles
of automated driving

319
00:14:21,940 --> 00:14:24,610
in the Phoenix, Arizona metropolitan area.

320
00:14:24,610 --> 00:14:27,700
Of the 18 total accidents that
occurred during the study,

321
00:14:27,700 --> 00:14:29,080
none were serious enough

322
00:14:29,080 --> 00:14:31,580
to expect significant injury or death.

323
00:14:31,580 --> 00:14:32,800
In Waymo's Safety Report,

324
00:14:32,800 --> 00:14:34,810
they found some types of accidents

325
00:14:34,810 --> 00:14:37,070
have been completely eliminated

326
00:14:37,070 --> 00:14:39,400
by this autonomous driving system,

327
00:14:39,400 --> 00:14:42,070
like the car doesn't go off the road

328
00:14:42,070 --> 00:14:44,580
and it doesn't hit stationary objects.

329
00:14:44,580 --> 00:14:47,810
Humans, humans do those things.

330
00:14:47,810 --> 00:14:51,220
If you look at the eight
significant accidents

331
00:14:51,220 --> 00:14:52,700
that happened with Waymo vehicles

332
00:14:52,700 --> 00:14:55,810
over the six million miles of driving,

333
00:14:55,810 --> 00:14:58,950
all eight of them involve a human driver

334
00:14:58,950 --> 00:15:01,070
of another vehicle doing something stupid,

335
00:15:01,070 --> 00:15:03,480
like driving on the wrong side of the road

336
00:15:03,480 --> 00:15:04,920
or running a red light

337
00:15:04,920 --> 00:15:07,870
or going through a stop
sign or failing to yield

338
00:15:07,870 --> 00:15:10,610
or going 20 miles per
hour over the speed limit.

339
00:15:10,610 --> 00:15:13,830
There were three incidents
involving Waymo vehicles

340
00:15:13,830 --> 00:15:17,570
and pedestrians, but in all three,

341
00:15:17,570 --> 00:15:21,330
the Waymo vehicle was
stationary and the pedestrian

342
00:15:21,330 --> 00:15:24,833
or cyclist/skateboarder
ran into the vehicle.

343
00:15:26,936 --> 00:15:29,650
Waymo also takes some
of that real-world data

344
00:15:29,650 --> 00:15:33,490
and they put it into simulations
and they tweak it a bit.

345
00:15:33,490 --> 00:15:36,230
So they try adding like
a bicyclist going fast

346
00:15:36,230 --> 00:15:39,660
or going slow, or they make
the car turn faster or slower.

347
00:15:39,660 --> 00:15:42,670
So they change all these
parameters and variables,

348
00:15:42,670 --> 00:15:44,960
and they see what the software will do.

349
00:15:44,960 --> 00:15:46,020
And they've trained the software

350
00:15:46,020 --> 00:15:49,480
on an additional 20
billion miles of driving,

351
00:15:49,480 --> 00:15:51,700
not on the road but in the simulation,

352
00:15:51,700 --> 00:15:55,283
so that's a thousand times
more experience again.

353
00:15:56,620 --> 00:15:58,870
The question for me is,

354
00:15:58,870 --> 00:16:01,680
when is stepping inside an autonomous car

355
00:16:01,680 --> 00:16:04,823
gonna feel the same as
stepping inside an elevator?

356
00:16:05,710 --> 00:16:07,820
Because I think that
time may be coming sooner

357
00:16:07,820 --> 00:16:09,150
than you think.

358
00:16:09,150 --> 00:16:10,570
I like the idea of this technology,

359
00:16:10,570 --> 00:16:11,930
but, honestly, getting in the car,

360
00:16:11,930 --> 00:16:14,470
I wasn't quite sure how I would feel.

361
00:16:14,470 --> 00:16:16,010
I was a little bit uncertain,

362
00:16:16,010 --> 00:16:19,240
but once I saw it just
handles so confidently...

363
00:16:19,240 --> 00:16:20,260
Like driving is one of those things

364
00:16:20,260 --> 00:16:21,440
that I feel like you can't hide

365
00:16:21,440 --> 00:16:22,640
whether you're a good or bad driver.

366
00:16:22,640 --> 00:16:24,280
It's just like, "Oh, what's gonna happen

367
00:16:24,280 --> 00:16:27,680
when there's a parked car or
a cyclist or a pedestrian,"

368
00:16:27,680 --> 00:16:30,310
and it just sort of handles
all those situations

369
00:16:30,310 --> 00:16:32,170
with such confidence and ease

370
00:16:32,170 --> 00:16:34,610
that I think I stopped thinking about it.

371
00:16:34,610 --> 00:16:36,900
- After they pass a couple
of your mental tests,

372
00:16:36,900 --> 00:16:38,230
you're like, "I'm good."

373
00:16:38,230 --> 00:16:40,130
I think, again, "I'll
be okay." (chuckling)

374
00:16:40,130 --> 00:16:41,900
- Yeah, yeah, I felt the same way.

375
00:16:41,900 --> 00:16:44,250
I think a lot of people
miss the bigger implications

376
00:16:44,250 --> 00:16:45,390
of what is achievable

377
00:16:45,390 --> 00:16:48,140
once fully autonomous
driving is commonplace.

378
00:16:48,140 --> 00:16:50,370
Riders with disabilities, seniors,

379
00:16:50,370 --> 00:16:52,620
and the blind can get around more easily.

380
00:16:52,620 --> 00:16:54,610
Transportation will get cheaper.

381
00:16:54,610 --> 00:16:56,700
Think of all the wasted value in the cars

382
00:16:56,700 --> 00:17:00,240
that spend over 95% of their time parked.

383
00:17:00,240 --> 00:17:03,250
We can regain a bunch
of time and feel happier

384
00:17:03,250 --> 00:17:07,390
because commuting and being
stuck in traffic sucks.

385
00:17:07,390 --> 00:17:09,010
We can reduce traffic

386
00:17:09,010 --> 00:17:11,840
because vehicles will have
better awareness of each other.

387
00:17:11,840 --> 00:17:13,110
You can imagine one day

388
00:17:13,110 --> 00:17:14,950
when all the cars are fully autonomous,

389
00:17:14,950 --> 00:17:18,540
they can execute a beautiful
ballet driving together,

390
00:17:18,540 --> 00:17:21,220
and when that time comes, we
can eliminate parking lots

391
00:17:21,220 --> 00:17:23,700
and add green spaces to our cities.

392
00:17:23,700 --> 00:17:24,533
And, most importantly,

393
00:17:24,533 --> 00:17:26,420
widespread adoption of autonomous cars

394
00:17:26,420 --> 00:17:29,010
could prevent tens of
thousands of fatalities

395
00:17:29,010 --> 00:17:30,490
in the U.S. alone.

396
00:17:30,490 --> 00:17:33,040
When do you think that
this will be a reality,

397
00:17:33,040 --> 00:17:34,200
but clearly it's coming,

398
00:17:34,200 --> 00:17:35,190
but when?
- Yeah.

399
00:17:35,190 --> 00:17:37,460
If you're talking about big cities,

400
00:17:37,460 --> 00:17:40,470
I'm hoping the next five years
will be really game changing.

401
00:17:40,470 --> 00:17:41,400
- [Derek] I'm excited to see it.

402
00:17:41,400 --> 00:17:43,110
- Me too, believe me. Believe me.

403
00:17:43,110 --> 00:17:45,210
I just don't want to
commute to work anymore.

404
00:17:45,210 --> 00:17:46,570
I would love to sit in the backseat,

405
00:17:46,570 --> 00:17:49,591
do my work on the way there and
on the way back. (chuckling)

406
00:17:49,591 --> 00:17:51,162
- [Derek] Or do something
fun, watch a movie.

407
00:17:51,162 --> 00:17:52,373
- Or sleep, yeah. (chuckling)

408
00:17:52,373 --> 00:17:53,206
(futuristic tones beeping)

409
00:17:53,206 --> 00:17:56,789
(frequency tone whistling)
